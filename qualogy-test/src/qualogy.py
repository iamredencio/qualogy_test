# -*- coding: utf-8 -*-
"""Kopie van Qualogy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14z_xXzKVvJazbOtQz7jNADaZnNBoq9mH
"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

import pandas as pd 
import numpy as np

from pandas.core.reshape.reshape import get_dummies


data_proc = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/colabdata/train_data.csv", index_col=0)

# Open file in notepad, to eliminate error 0x83 utf
test_data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/colabdata/TestDataAccomodation.csv", encoding="utf-8", index_col=0)
test_data

"""#Analyze data
## Half of the data is categorial other half is continuous
"""

data_proc.info()

"""#Fill in empty spaces
## Sample randomly
"""

#Random Sampling missng values
data_proc = data_proc.apply(lambda x: np.where(x.isnull(), x.dropna().sample(len(x), replace=True), x))
#Random Sampling missng values
test_data = test_data.apply(lambda x: np.where(x.isnull(), x.dropna().sample(len(x), replace=True), x))

"""#Split data into train, test and validation sets
## prepare for cross validation to accommodate skewed data.
"""

data = pd.get_dummies(data_proc.drop('AcomType', axis=1))
data["AcomType"] = data_proc["AcomType"]
# data
feature = data.drop('AcomType', axis=1)
target = data['AcomType']

X = feature.values
y = target.values

X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=41)
X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, random_state=3)

print("Training set{}\nValidation set{}\nTest set{}".format(X_train.shape, X_valid.shape, X_test.shape))

"""# Standardize data"""

scaler = MinMaxScaler()
scaler.fit(X_trainval)

X_trainval_scaled = scaler.transform(X_trainval)
X_test_scaled = scaler.transform(X_test)
X_train_scaled = scaler.transform(X_train)
X_valid_scaled = scaler.transform(X_valid)

test_data = pd.get_dummies(test_data)

scaler = MinMaxScaler()
scaler.fit(test_data)

test_data_scaled = scaler.transform(test_data)
# test_data_scaled


"""#Support Vector Machine"""

def support_vector_machine(X_train_scaled,  y_train, X_valid_scaled, y_valid, X_trainval_scaled,  y_trainval, X_test_scaled, y_test):
    best_score = 0

    for C in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]:
      for gamma in ["auto", "scale"]:
        for kernel in ["linear", "poly", "rbf", "sigmoid"]:
          svc = SVC(kernel=kernel, C=C, gamma=gamma)
          svc.fit(X_train_scaled,  y_train)
          score = svc.score(X_valid_scaled, y_valid)

          if score > best_score:
            best_score = score
            best_parameters = {"C":C,
                              'gamma': gamma,
                              'kernel': kernel}

    svc = SVC(**best_parameters)
    svc.fit(X_trainval_scaled,  y_trainval)
    train_score = svc.score(X_trainval_scaled,  y_trainval)
    test_score = svc.score(X_test_scaled, y_test)

    return best_score, best_parameters, train_score, test_score

support_vector_machine(X_train_scaled,  y_train, X_valid_scaled, y_valid, X_trainval_scaled,  y_trainval, X_test_scaled, y_test)

"""#Logistic regression"""

def logistic_regression(X_train_scaled,  y_train, X_valid_scaled, y_valid, X_trainval_scaled,  y_trainval, X_test_scaled, y_test):
    best_score = 0

    for C in [0.001, 0.01, 0.1, 1, 10, 100]:
      for solver in ["newton-cg", "sag", "saga", "lbfgs"]:
          logreg = LogisticRegression(max_iter=5000, multi_class='multinomial', C=C,
                                      solver=solver, penalty='l2')
          logreg.fit(X_train_scaled,  y_train)
          score = logreg.score(X_valid_scaled, y_valid)

          if score > best_score:
            best_score = score
            best_parameters = {"C":C,
                              'solver': solver}

    logreg = LogisticRegression(**best_parameters)
    logreg.fit(X_trainval_scaled,  y_trainval)
    train_score = logreg.score(X_trainval_scaled,  y_trainval)
    test_score = logreg.score(X_test_scaled, y_test)

    return best_score, best_parameters, train_score, test_score

logistic_regression(X_train_scaled,  y_train, X_valid_scaled, y_valid, X_trainval_scaled,  y_trainval, X_test_scaled, y_test)

"""#RandomForestClassifier"""

def random_forest_classifier(X_train_scaled,  y_train, X_valid_scaled, y_valid, X_trainval_scaled,  y_trainval, X_test_scaled, y_test):
    best_score = 0

    for criterion in ['gini', 'entropy']:
      for max_depth in range(1,11):
        for max_features in ["auto", "sqrt", "log2"]:
          rf = RandomForestClassifier(criterion=criterion, max_features=max_features, max_depth=max_depth)
          rf.fit(X_train_scaled,  y_train)
          score = rf.score(X_valid_scaled, y_valid)

          if score > best_score:
            best_score = score
            best_parameters = {"max_features":max_features,
                              'max_depth': max_depth,
                              'criterion': criterion}

    rf = RandomForestClassifier(**best_parameters)
    rf.fit(X_trainval_scaled,  y_trainval)
    train_score = rf.score(X_trainval_scaled,  y_trainval)
    test_score = rf.score(X_test_scaled, y_test)

    return best_score, best_parameters, train_score, test_score

random_forest_classifier(X_train_scaled,  y_train, X_valid_scaled, y_valid, X_trainval_scaled,  y_trainval, X_test_scaled, y_test)

"""# https://medium.com/nmc-techblog/easy-drag-and-drop-in-react-22778b30ba37
# https://medium.com/analytics-vidhya/classification-method-for-estimating-the-numbers-of-rings-of-abalone-bb13264dd186

"""